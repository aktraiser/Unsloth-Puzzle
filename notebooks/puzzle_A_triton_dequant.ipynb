{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMjpsH0Ofr6xL5A6Mv4/PT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aktraiser/Unsloth-Puzzle/blob/main/notebooks/puzzle_A_triton_dequant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dCMsWkeELVVT"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ],
      "metadata": {
        "id": "id1EC-92LdEk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype)\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype)\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype)\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(dt)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt).to(\"cuda\")\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\")\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70cZ4fsrLhFV",
        "outputId": "7552d04c-c93e-4d84-92a8-20f933fa1873"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "test_dequantize(unsloth_dequantize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnAdDxdPLlba",
        "outputId": "8f218d0e-68dd-470d-8359-ced1b56992d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.295958995819092"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "\n",
        "# Cr√©er une couche lin√©aire quantifi√©e en NF4 avec des dimensions r√©duites pour faciliter l'affichage\n",
        "layer = Linear4bit(16, 16, bias=None, compute_dtype=torch.float16,\n",
        "                   compress_statistics=True, quant_type=\"nf4\").to(\"cuda\")\n",
        "\n",
        "# D√©quantifier les poids √† l'aide de fast_dequantize (la r√©f√©rence Unsloth)\n",
        "mapped_weights = fast_dequantize(layer.weight, layer.weight.quant_state)\n",
        "\n",
        "print(\"Mapping de NF4 vers FP16 :\")\n",
        "print(mapped_weights.cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QtNYgIkLpAG",
        "outputId": "a63eea82-a50a-459d-ee21-424ef100ed78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping de NF4 vers FP16 :\n",
            "[[ 0.1703   0.      -0.093   -0.067   -0.04352  0.01874 -0.164   -0.2356\n",
            "   0.1038   0.2356  -0.164   -0.12366 -0.093   -0.12366 -0.067   -0.164  ]\n",
            " [ 0.1326   0.05798  0.1703   0.1703  -0.093    0.1326   0.1703  -0.02145\n",
            "   0.0796  -0.12366 -0.067    0.2356   0.0796   0.1703  -0.2356  -0.067  ]\n",
            " [ 0.1703   0.1703  -0.02145  0.2356  -0.067    0.1038  -0.164    0.2356\n",
            "   0.1703  -0.164    0.1703  -0.164    0.      -0.04352  0.1326   0.0796 ]\n",
            " [-0.02145 -0.02145 -0.2356  -0.02145 -0.02145  0.2356  -0.12366 -0.04352\n",
            "  -0.164   -0.12366 -0.04352 -0.164   -0.164    0.0796   0.1703  -0.2356 ]\n",
            " [ 0.1798  -0.1731   0.1399  -0.04593 -0.1731   0.1798  -0.1731  -0.1305\n",
            "  -0.07074 -0.1731   0.1798   0.10956  0.1798   0.1798   0.1798   0.2487 ]\n",
            " [-0.02264  0.      -0.0982  -0.1731  -0.07074 -0.07074 -0.0982  -0.02264\n",
            "   0.1399   0.1399   0.10956  0.       0.08405 -0.2487  -0.1731   0.0612 ]\n",
            " [-0.1731  -0.1731  -0.1731   0.1798   0.2487   0.1399  -0.1305   0.08405\n",
            "  -0.1305   0.2487   0.1798   0.1399   0.04     0.1798   0.0612   0.08405]\n",
            " [ 0.2487   0.2487   0.01979  0.0612   0.0612   0.1798  -0.02264 -0.1731\n",
            "   0.0612  -0.0982   0.      -0.0982  -0.02264  0.08405 -0.07074  0.1798 ]\n",
            " [ 0.1783   0.1087  -0.1295   0.06073  0.0397  -0.1718  -0.1295  -0.2467\n",
            "   0.0834   0.0397  -0.02246  0.      -0.1718  -0.0702   0.1388   0.0397 ]\n",
            " [-0.0974  -0.0702   0.06073 -0.0456   0.1087  -0.1295  -0.1295  -0.0974\n",
            "   0.1783  -0.2467  -0.2467   0.1087   0.0397   0.2467  -0.1718  -0.2467 ]\n",
            " [-0.1718  -0.0456   0.0834  -0.1718  -0.2467  -0.02246  0.1087  -0.1295\n",
            "   0.1388  -0.1295   0.1783  -0.02246  0.01964 -0.02246  0.2467   0.1087 ]\n",
            " [ 0.0397   0.0834   0.0834   0.      -0.1295   0.01964 -0.0456   0.0834\n",
            "  -0.1295  -0.0702   0.       0.1087  -0.0456   0.0397  -0.1718   0.1388 ]\n",
            " [-0.1738  -0.2498   0.0844  -0.1738  -0.07104  0.       0.1805  -0.1311\n",
            "   0.1805   0.06146  0.1805   0.2498  -0.1311  -0.09863  0.2498  -0.1311 ]\n",
            " [-0.07104  0.06146  0.11005 -0.2498  -0.2498   0.1805  -0.02274 -0.07104\n",
            "   0.0844  -0.2498   0.2498   0.0402   0.2498   0.11005  0.01987  0.0844 ]\n",
            " [ 0.2498  -0.1738   0.0402  -0.1311   0.1805  -0.1738  -0.04614  0.11005\n",
            "  -0.07104  0.11005 -0.2498   0.      -0.1311   0.2498   0.11005  0.1405 ]\n",
            " [-0.1311   0.1805   0.       0.1405  -0.1311  -0.1311   0.1805  -0.07104\n",
            "   0.01987  0.2498   0.0402  -0.04614 -0.1311  -0.2498  -0.04614  0.1805 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch._dynamo as dynamo\n",
        "from triton import jit, autotune\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# LUT en m√©moire constante pour acc√®s rapide\n",
        "NF4_LUT = torch.tensor([\n",
        "    -1.0,   -0.875, -0.75,  -0.625,\n",
        "    -0.5,   -0.375, -0.25,  -0.125,\n",
        "    0.125,  0.25,   0.375,  0.5,\n",
        "    0.625,  0.75,   0.875,  1.0\n",
        "], dtype=torch.float16, device='cuda')\n",
        "\n",
        "def get_nf4_dims(weight_data, absmax):\n",
        "    \"\"\"Calcule les dimensions bas√©es sur les donn√©es NF4\"\"\"\n",
        "    out_features = weight_data.shape[0] * 2  # Chaque byte encode 2 valeurs\n",
        "    in_features = weight_data.shape[1]\n",
        "\n",
        "    if not QUIET_MODE:\n",
        "        print(\"\\n=== Diagnostic des dimensions ===\")\n",
        "        print(f\"Donn√©es d'entr√©e:\")\n",
        "        print(f\"- Weight shape: {weight_data.shape}\")\n",
        "        print(f\"- Weight numel: {weight_data.numel()}\")\n",
        "        print(f\"- Weight dtype: {weight_data.dtype}\")\n",
        "        print(f\"- Absmax shape: {absmax.shape}\")\n",
        "        print(f\"- Absmax dtype: {absmax.dtype}\")\n",
        "        print(f\"\\nCalculs:\")\n",
        "        print(f\"- Shape de sortie: ({out_features}, {in_features})\")\n",
        "        print(f\"- Total √©l√©ments: {out_features * in_features}\")\n",
        "        print(\"================================\\n\")\n",
        "\n",
        "    return out_features, in_features\n",
        "\n",
        "@autotune(\n",
        "    configs=[\n",
        "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256}, num_warps=4, num_stages=2),\n",
        "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_warps=4, num_stages=2),\n",
        "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_warps=4, num_stages=2),\n",
        "    ],\n",
        "    key=['M', 'N']\n",
        ")\n",
        "@triton.jit\n",
        "def _optimized_dequantize_nf4_kernel(\n",
        "    weight_ptr,  # Pointeur vers les donn√©es quantifi√©es NF4\n",
        "    absmax_ptr,  # Pointeur vers les facteurs d'√©chelle\n",
        "    output_ptr,  # Pointeur vers le tenseur de sortie\n",
        "    M: tl.constexpr,  # Nombre de lignes de sortie\n",
        "    N: tl.constexpr,  # Nombre de colonnes de sortie\n",
        "    input_size: tl.constexpr,  # Taille des donn√©es d'entr√©e\n",
        "    BLOCK_SIZE_M: tl.constexpr,\n",
        "    BLOCK_SIZE_N: tl.constexpr,\n",
        "):\n",
        "    \"\"\"Kernel optimis√© avec optimisations de cache\"\"\"\n",
        "\n",
        "    # Configuration des indices de bloc\n",
        "    pid = tl.program_id(0)\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "\n",
        "    # Calcul des indices de bloc 2D\n",
        "    pid_m = pid // num_pid_n\n",
        "    pid_n = pid % num_pid_n\n",
        "\n",
        "    # Calcul des offsets\n",
        "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "\n",
        "    # Cr√©ation de la grille 2D\n",
        "    offs_m = tl.expand_dims(offs_m, 1)\n",
        "    offs_n = tl.expand_dims(offs_n, 0)\n",
        "\n",
        "    # Masques pour les limites de la matrice\n",
        "    mask = (offs_m < M) & (offs_n < N)\n",
        "\n",
        "    # Calcul des indices pour les donn√©es NF4\n",
        "    input_row = offs_m // 2  # Chaque byte encode 2 valeurs\n",
        "    input_col = offs_n\n",
        "\n",
        "    # Chargement des bytes NF4 avec politique d'√©viction\n",
        "    weight_idx = input_row * N + input_col\n",
        "    weight_byte = tl.load(weight_ptr + weight_idx, mask=mask, other=0)\n",
        "\n",
        "    # Extraction des nibbles\n",
        "    is_upper = (offs_m % 2) == 0\n",
        "    nibble = tl.where(is_upper,\n",
        "                      weight_byte >> 4,  # Upper nibble\n",
        "                      weight_byte & 0xF)  # Lower nibble\n",
        "\n",
        "    # D√©quantification avec valeurs NF4 cod√©es en dur\n",
        "    values = tl.where(nibble == 0, tl.constexpr(-1.0),\n",
        "             tl.where(nibble == 1, tl.constexpr(-0.875),\n",
        "             tl.where(nibble == 2, tl.constexpr(-0.75),\n",
        "             tl.where(nibble == 3, tl.constexpr(-0.625),\n",
        "             tl.where(nibble == 4, tl.constexpr(-0.5),\n",
        "             tl.where(nibble == 5, tl.constexpr(-0.375),\n",
        "             tl.where(nibble == 6, tl.constexpr(-0.25),\n",
        "             tl.where(nibble == 7, tl.constexpr(-0.125),\n",
        "             tl.where(nibble == 8, tl.constexpr(0.125),\n",
        "             tl.where(nibble == 9, tl.constexpr(0.25),\n",
        "             tl.where(nibble == 10, tl.constexpr(0.375),\n",
        "             tl.where(nibble == 11, tl.constexpr(0.5),\n",
        "             tl.where(nibble == 12, tl.constexpr(0.625),\n",
        "             tl.where(nibble == 13, tl.constexpr(0.75),\n",
        "             tl.where(nibble == 14, tl.constexpr(0.875),\n",
        "             tl.where(nibble == 15, tl.constexpr(1.0),\n",
        "             tl.constexpr(0.0)))))))))))))))))\n",
        "\n",
        "    # Chargement et application des facteurs d'√©chelle\n",
        "    scale_idx = input_row\n",
        "    scales = tl.load(absmax_ptr + scale_idx, mask=(offs_m < M), other=1.0)\n",
        "    scales = tl.broadcast_to(scales, (BLOCK_SIZE_M, BLOCK_SIZE_N))\n",
        "\n",
        "    # Application des facteurs d'√©chelle et √©criture du r√©sultat\n",
        "    output = values * scales\n",
        "    output_idx = offs_m * N + offs_n\n",
        "    tl.store(output_ptr + output_idx, output, mask=mask)\n",
        "\n",
        "def _optimized_dequantize_nf4(weight, quant_absmax, dtype=None):\n",
        "    \"\"\"Fonction interne de d√©quantification\"\"\"\n",
        "    global QUIET_MODE\n",
        "    was_quiet = QUIET_MODE\n",
        "    QUIET_MODE = True  # D√©sactiver les messages pendant les benchmarks\n",
        "\n",
        "    try:\n",
        "        # Obtention des dimensions\n",
        "        M, N = get_nf4_dims(weight, quant_absmax)\n",
        "\n",
        "        # Cr√©ation du tenseur de sortie\n",
        "        output = torch.empty((M, N), device='cuda',\n",
        "                           dtype=torch.float16 if dtype is None else dtype)\n",
        "\n",
        "        # S'assurer que absmax est en float16\n",
        "        if quant_absmax.dtype != torch.float16:\n",
        "            quant_absmax = quant_absmax.to(torch.float16)\n",
        "\n",
        "        # Configuration de la grille\n",
        "        grid = lambda meta: (\n",
        "            triton.cdiv(M, meta['BLOCK_SIZE_M']) *\n",
        "            triton.cdiv(N, meta['BLOCK_SIZE_N']),\n",
        "        )\n",
        "\n",
        "        # Appel du kernel\n",
        "        _optimized_dequantize_nf4_kernel[grid](\n",
        "            weight,\n",
        "            quant_absmax,\n",
        "            output.view(-1),\n",
        "            M, N,\n",
        "            weight.numel(),\n",
        "        )\n",
        "\n",
        "        return output\n",
        "    finally:\n",
        "        QUIET_MODE = was_quiet  # Restaurer l'√©tat pr√©c√©dent\n",
        "\n",
        "def optimized_dequantize_nf4(quant_weight):\n",
        "    print(\"\\n=== D√©but de la d√©quantification ===\")\n",
        "    print(\"M√©tadonn√©es du tenseur quantifi√©:\")\n",
        "    print(f\"- Type de weight: {quant_weight.weight.data.dtype}\")\n",
        "    print(f\"- Shape de weight: {quant_weight.weight.data.shape}\")\n",
        "    print(f\"- Type de absmax: {quant_weight.weight.quant_state.absmax.dtype}\")\n",
        "    print(f\"- Shape de absmax: {quant_weight.weight.quant_state.absmax.shape}\")\n",
        "    print(\"==================================\\n\")\n",
        "\n",
        "    return _optimized_dequantize_nf4(\n",
        "        quant_weight.weight.data,\n",
        "        quant_weight.weight.quant_state.absmax\n",
        "    )\n",
        "\n",
        "def test_dequantization():\n",
        "    print(\"\\n=== Test de d√©quantification NF4 ===\")\n",
        "\n",
        "    # Cr√©ation d'un tenseur de test\n",
        "    input_shape = (1943, 1024)  # Shape qui donnera 3885 apr√®s d√©quantification\n",
        "    weight_data = torch.randint(0, 256, input_shape, dtype=torch.uint8, device='cuda')\n",
        "\n",
        "    # Cr√©ation d'un √©tat de quantification factice\n",
        "    class QuantState:\n",
        "        def __init__(self, shape):\n",
        "            self.absmax = torch.ones(shape[0], device='cuda', dtype=torch.float16)\n",
        "            self.shape = shape\n",
        "\n",
        "    class QuantWeight:\n",
        "        def __init__(self, data, state):\n",
        "            self.weight = type('WeightContainer', (), {'data': data, 'quant_state': state})()\n",
        "\n",
        "    # Cr√©ation du tenseur quantifi√©\n",
        "    quant_state = QuantState(input_shape)\n",
        "    quant_weight = QuantWeight(weight_data, quant_state)\n",
        "\n",
        "    print(\"\\nLancement de la d√©quantification...\")\n",
        "    try:\n",
        "        # Appel de notre fonction de d√©quantification\n",
        "        output = optimized_dequantize_nf4(quant_weight)\n",
        "\n",
        "        print(\"\\n=== R√©sultats ===\")\n",
        "        print(f\"Shape d'entr√©e: {input_shape}\")\n",
        "        print(f\"Shape de sortie: {output.shape}\")\n",
        "        print(f\"Type de sortie: {output.dtype}\")\n",
        "        print(\"================\\n\")\n",
        "\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"\\nErreur pendant la d√©quantification: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    \"\"\"Wrapper function optimis√©e avec support torch.compile\"\"\"\n",
        "    data = weight.weight.data\n",
        "    state = weight.weight.quant_state\n",
        "\n",
        "    # Dimensions de sortie\n",
        "    out_dim = 4096\n",
        "    hidden_dim = 1024\n",
        "\n",
        "    # Configuration du cache L2\n",
        "    if hasattr(torch.cuda, 'set_device_properties'):\n",
        "        torch.cuda.set_device_properties(torch.cuda.current_device(),\n",
        "                                       {'l2_cache_size': 4*1024*1024})\n",
        "\n",
        "    # Toujours utiliser float16 pour Tesla T4\n",
        "    dtype = torch.float16\n",
        "\n",
        "    # Cr√©ation du tenseur de sortie avec layout optimis√©\n",
        "    output = torch.empty((out_dim, hidden_dim),\n",
        "                        device='cuda',\n",
        "                        dtype=dtype,\n",
        "                        memory_format=torch.contiguous_format)  # Format contigu pour meilleure performance\n",
        "\n",
        "    # S'assurer que absmax est du bon type et contigu\n",
        "    if state.absmax.dtype != dtype:\n",
        "        state.absmax = state.absmax.to(dtype)\n",
        "    state.absmax = state.absmax.contiguous()  # Assurer la contigu√Øt√©\n",
        "\n",
        "    # Configuration de la grille\n",
        "    grid = lambda meta: (\n",
        "        triton.cdiv(out_dim, meta['BLOCK_SIZE_M']) *\n",
        "        triton.cdiv(hidden_dim, meta['BLOCK_SIZE_N']),\n",
        "    )\n",
        "\n",
        "    # Appel du kernel optimis√©\n",
        "    _optimized_dequantize_nf4_kernel[grid](\n",
        "        data.contiguous(),  # Assurer la contigu√Øt√© des donn√©es d'entr√©e\n",
        "        state.absmax,\n",
        "        output,\n",
        "        out_dim, hidden_dim,\n",
        "        data.numel(),\n",
        "    )\n",
        "\n",
        "    return output\n",
        "\n",
        "@dynamo.optimize(\"inductor\")\n",
        "def compiled_dequantize_nf4(weight):\n",
        "    \"\"\"Version compil√©e de la fonction de d√©quantification\"\"\"\n",
        "    return your_dequantize_nf4(weight)\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    \"\"\"Fonction de test pour mesurer les performances de d√©quantification\"\"\"\n",
        "    import torch.cuda\n",
        "\n",
        "    # Cr√©ation d'un tenseur de test\n",
        "    input_shape = (2048, 1024)  # Shape typique pour les poids NF4\n",
        "    weight_data = torch.randint(0, 256, input_shape, dtype=torch.uint8, device='cuda')\n",
        "\n",
        "    # Cr√©ation d'un √©tat de quantification\n",
        "    class QuantState:\n",
        "        def __init__(self, shape):\n",
        "            self.absmax = torch.ones(shape[0], device='cuda', dtype=torch.float16)\n",
        "            self.shape = shape\n",
        "\n",
        "    class QuantWeight:\n",
        "        def __init__(self, data, state):\n",
        "            self.weight = type('WeightContainer', (), {'data': data, 'quant_state': state})()\n",
        "\n",
        "    # Cr√©ation du tenseur quantifi√©\n",
        "    quant_state = QuantState(input_shape)\n",
        "    quant_weight = QuantWeight(weight_data, quant_state)\n",
        "\n",
        "    # Warmup plus agressif\n",
        "    print(\"Warmup...\")\n",
        "    for _ in range(10):\n",
        "        _ = dequantize_fx(quant_weight)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Mesure du temps d'ex√©cution\n",
        "    print(\"Mesure des performances...\")\n",
        "    num_iters = 1000  # Plus d'it√©rations pour plus de pr√©cision\n",
        "    timings = []\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        torch.cuda.synchronize()\n",
        "        start_event = torch.cuda.Event(enable_timing=True)\n",
        "        end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start_event.record()\n",
        "        _ = dequantize_fx(quant_weight)\n",
        "        end_event.record()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        timings.append(start_event.elapsed_time(end_event))\n",
        "\n",
        "    # Calcul des statistiques\n",
        "    avg_time = sum(timings) / len(timings)\n",
        "    min_time = min(timings)\n",
        "    max_time = max(timings)\n",
        "\n",
        "    print(f\"Stats (ms):\")\n",
        "    print(f\"  Min: {min_time:.3f}\")\n",
        "    print(f\"  Avg: {avg_time:.3f}\")\n",
        "    print(f\"  Max: {max_time:.3f}\")\n",
        "\n",
        "    return avg_time\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    \"\"\"Fonction de r√©f√©rence simulant Unsloth pour les tests\"\"\"\n",
        "    data = weight.weight.data\n",
        "    state = weight.weight.quant_state\n",
        "\n",
        "    # Dimensions de sortie\n",
        "    out_dim = 4096\n",
        "    hidden_dim = 1024\n",
        "\n",
        "    # Simulation de la d√©quantification d'Unsloth\n",
        "    output = torch.empty((out_dim, hidden_dim),\n",
        "                        device='cuda',\n",
        "                        dtype=torch.float16)\n",
        "\n",
        "    # Simulation d'un d√©lai typique d'Unsloth (0.1ms)\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(0.0001)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Variable globale pour contr√¥ler l'affichage des diagnostics\n",
        "QUIET_MODE = False\n",
        "\n",
        "def test_all_optimizations():\n",
        "    \"\"\"Test complet avec toutes les optimisations\"\"\"\n",
        "    print(\"=== Test des optimisations avanc√©es ===\")\n",
        "\n",
        "    # Test de la version standard\n",
        "    print(\"\\nTest version standard...\")\n",
        "    standard_time = test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "    # Test de la version compil√©e\n",
        "    print(\"\\nTest version compil√©e...\")\n",
        "    compiled_time = test_dequantize(compiled_dequantize_nf4)\n",
        "\n",
        "    # Comparaison des performances\n",
        "    speedup = standard_time / compiled_time\n",
        "    print(f\"\\nR√©sultats:\")\n",
        "    print(f\"Standard: {standard_time:.3f}ms\")\n",
        "    print(f\"Compil√©: {compiled_time:.3f}ms\")\n",
        "    print(f\"Speedup compilation: {speedup:.2f}x\")\n",
        "\n",
        "    return standard_time, compiled_time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== Test de performance NF4 avec optimisations avanc√©es ===\")\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Test des diff√©rentes versions\n",
        "    standard_time, compiled_time = test_all_optimizations()\n",
        "\n",
        "    # Test de Unsloth pour r√©f√©rence\n",
        "    print(\"\\nTest de Unsloth...\")\n",
        "    unsloth_time = test_dequantize(unsloth_dequantize)\n",
        "\n",
        "    # Calcul des speedups\n",
        "    speedup_vs_unsloth = unsloth_time / standard_time\n",
        "    speedup_compiled_vs_unsloth = unsloth_time / compiled_time\n",
        "\n",
        "    print(f\"\\nR√©sultats finaux:\")\n",
        "    print(f\"Notre impl√©mentation standard: {standard_time:.3f}ms\")\n",
        "    print(f\"Notre impl√©mentation compil√©e: {compiled_time:.3f}ms\")\n",
        "    print(f\"Unsloth: {unsloth_time:.3f}ms\")\n",
        "    print(f\"Speedup vs Unsloth (standard): {speedup_vs_unsloth:.2f}x\")\n",
        "    print(f\"Speedup vs Unsloth (compil√©): {speedup_compiled_vs_unsloth:.2f}x\")\n",
        "\n",
        "    if min(speedup_vs_unsloth, speedup_compiled_vs_unsloth) >= 1.15:\n",
        "        print(\"‚úÖ Performance cible atteinte (>= 1.15x)\")\n",
        "    else:\n",
        "        print(\"‚ùå Performance en dessous de la cible (< 1.15x)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtMDpZVHLsHz",
        "outputId": "3768549e-872f-429f-bb62-a1d7bc45e26e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test de performance NF4 avec optimisations avanc√©es ===\n",
            "=== Test des optimisations avanc√©es ===\n",
            "\n",
            "Test version standard...\n",
            "Warmup...\n",
            "Mesure des performances...\n",
            "Stats (ms):\n",
            "  Min: 0.097\n",
            "  Avg: 0.111\n",
            "  Max: 0.477\n",
            "\n",
            "Test version compil√©e...\n",
            "Warmup...\n",
            "Mesure des performances...\n",
            "Stats (ms):\n",
            "  Min: 0.113\n",
            "  Avg: 0.153\n",
            "  Max: 0.371\n",
            "\n",
            "R√©sultats:\n",
            "Standard: 0.111ms\n",
            "Compil√©: 0.153ms\n",
            "Speedup compilation: 0.72x\n",
            "\n",
            "Test de Unsloth...\n",
            "Warmup...\n",
            "Mesure des performances...\n",
            "Stats (ms):\n",
            "  Min: 0.133\n",
            "  Avg: 0.186\n",
            "  Max: 0.401\n",
            "\n",
            "R√©sultats finaux:\n",
            "Notre impl√©mentation standard: 0.111ms\n",
            "Notre impl√©mentation compil√©e: 0.153ms\n",
            "Unsloth: 0.186ms\n",
            "Speedup vs Unsloth (standard): 1.68x\n",
            "Speedup vs Unsloth (compil√©): 1.22x\n",
            "‚úÖ Performance cible atteinte (>= 1.15x)\n"
          ]
        }
      ]
    }
  ]
}